{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Modelling"
      ],
      "metadata": {
        "id": "taAjs_BSyn3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "################################# COMPONENTS ######################################\n",
        "\n",
        "\n",
        "class MultiheadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,d_vector,heads):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    self.d_model=d_model\n",
        "    self.d_vector=d_vector\n",
        "    self.heads=heads\n",
        "\n",
        "    self.Wk = tf.keras.layers.Dense(d_vector) ### => (Batch_size x sequence_length x d_vector)\n",
        "\n",
        "    ### Theory states for every input sequence of dimension (sequence_length x d_model) (Xi),\n",
        "    ### we multiply by a weight Wk, Wq, Wv to obtain Key, Query and Value vectors of dimension (sequence_length x vector_dimension), where the weights are trainable parameters\n",
        "    ### For batch learning the shape becomes (batch_size x sequence_length x d_model)\n",
        "\n",
        "    ### In a dense layer, there is a kernel which is essentially a weight matrix (Wk), multiplied by the input, to create the feature vector.\n",
        "    ### normally, we enter 2D shapes to a dense layer : (batch_size x input_vector_length) which multiplied by kernel (input_vector_length x feature_vector_length(no. of nodes -> 64 here))\n",
        "    ### becomes (batch_size x feature vector length)\n",
        "\n",
        "    ### But the input here is Xi of 3D dimension of (batch_size x sequence_length x d_model) => In case, we send data with rank more than 2 to a dense layer\n",
        "    ### it flattens the data -> ((batch_size * sequence_length) x d_model) -> performs the multiplication to create -> ((batch_size * sequence_length) x vector_dimension)\n",
        "    ### And then reconverts it into 3D -> (batch_size x sequence_length x vector_dimension) => So, we get the vectors.\n",
        "\n",
        "    self.Wq = tf.keras.layers.Dense(d_vector)\n",
        "    self.Wv = tf.keras.layers.Dense(d_vector)\n",
        "\n",
        "    self.Wo = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    ### The final_layer\n",
        "\n",
        "\n",
        "\n",
        "  def dot_product_attention(self, Query, Key, Value, Mask: None):\n",
        "    # Mask only for decoder\n",
        "\n",
        "    KQ_dot_product = tf.matmul(Key, Query, transpose_b = True) # K.Q\n",
        "\n",
        "    # K:(Sequence_length x vector_dimension)\n",
        "    # Q:(Sequence_length x vector_dimension) ## For each input for each head for each input in batch\n",
        "\n",
        "    ### K and Q: (Batch_size x head x Sequence_Length x Vector_dimension)\n",
        "    ### Q after transpose: (Batch_size x head x Vector_dimension x Sequence Length)\n",
        "\n",
        "    ### Note: For any N-Dimensional matrix multiplication the last 2 dimensions are switched and operations are there. So,\n",
        "\n",
        "    ### For 4D matrices : last index of the first matrix should match the second last index of the second matrix.\n",
        "\n",
        "    ### K.Q => (Batch_size x head x Sequence_Length x Sequence Length) => For an element of a batch for each head -> attention of all the other words of the sequence for each word of the sequence\n",
        "\n",
        "    Normalized_dot_product = KQ_dot_product/ np.sqrt(vector_dimensions) ## Normalized: K.Q / sqrt(dk)\n",
        "    ### Normalized_dot_product => (Batch_size x head x Sequence_Length x Sequence_Length)\n",
        "\n",
        "    if Mask is not None:\n",
        "      Normalized_dot_product = Normalized_dot_product + (1. - Mask) *(-1e9) ## For decoder is mask is there, we make it 0.\n",
        "      ### Mask dimension -> (Batch_size x head x Sequence_Length x Sequence_Length)\n",
        "\n",
        "    Attention_weights = tf.nn.softmax(Normalized_dot_product, axis = -1)\n",
        "    ### Attention weights -> (Batch_size x head x Sequence_Length x Sequence_Length) -> the last axis contains the importance of all the other words on the word\n",
        "    ### Softmax sums everything upto 1\n",
        "\n",
        "\n",
        "    Attention_layer_output = tf.matmul(Attention_weights, Value)\n",
        "    ### V: (Batch_size x Sequence_Length x Vector_dimension)\n",
        "\n",
        "    ### Multiplication: (Batch_size x head x Sequence_Length x Sequence_Length) x (Batch_size x Sequence_Length x Vector_dimension)\n",
        "    ### => The last axis of first matrix and second-last axis of the second matrix (As explained above) => Resultant (Batch_size x head x Sequence_Length x Vector_dimension)\n",
        "\n",
        "    ### For 2D =>  (Sequence_Length x Sequence_Length) x (Sequence_Length x Vector_dimension) => (Sequence_Length x Vector_dimension)\n",
        "    ### Result => The final Value vector for each word in the sequence\n",
        "\n",
        "    ### Attention_layer_output => (Batch_size x head x Sequence_Length x Vector_dimension)\n",
        "    ### Above result for the full batch for each head\n",
        "\n",
        "    return Attention_layer_output\n",
        "\n",
        "  def project_to_heads(self, Query, Key, Value):\n",
        "\n",
        "    ## Query, value, key Shape: (Batch size x sequence length x vector_dimension)\n",
        "\n",
        "    ## Projecting the above tensors for each head -> We need the shape -> (Batch_size x Head x sequence length x vector_dimension)\n",
        "\n",
        "    ## We can only add an extra dimension on the last axis, as, dimension adding actually projects the key, query and value vectors in the added axis\n",
        "    ## in such a way that all the components adds up to give the initial query, key and value vector. so, reshaping to add heads to all the vectors\n",
        "\n",
        "    Query = tf.reshape(Query, shape=(Query.shape[0], Query.shape[1], No_of_heads, -1))\n",
        "    Key = tf.reshape(Key, shape=(Key.shape[0], Key.shape[1], No_of_heads, -1))\n",
        "    Value = tf.reshape(Value, shape=(Value.shape[0], Value.shape[1], No_of_heads, -1))\n",
        "\n",
        "    ## Obtained shape => (Batch_size x sequence_length x head x vector_dimension)\n",
        "\n",
        "    ### As each of them are different projections on different planes of the head axis, all the vectors are different.\n",
        "\n",
        "    ### Transposing to required shape\n",
        "\n",
        "    Query = tf.transpose(Query, perm=[0,2,1,3])\n",
        "    Key = tf.transpose(Key, perm=[0,2,1,3])\n",
        "    Value = tf.transpose(Value, perm=[0,2,1,3])\n",
        "\n",
        "    ## Obtained shape => (Batch_size x head x sequence_length x vector_dimension)\n",
        "\n",
        "    return Query, Key, Value\n",
        "\n",
        "\n",
        "  def call(self,Query_input,Key_input,Value_input,Mask):\n",
        "\n",
        "      ### We need Xi to create Ki,Qi and Vi so, why we are taking 3 different values here?\n",
        "      ### For encoder, it is true, each of the 3 input is Xi only, but for decoder, there are 3 cases,\n",
        "      ### where,\n",
        "      ### 1. Query and key inputs come from encoder feedback and value comes from the pervious decoder layer and so on. So, the value inputs can be different.\n",
        "\n",
        "\n",
        "      ## Finding all the vectors Query, key and value vectors: (Batch_size x sequence_length x vector_dimension)\n",
        "      Q = self.Wq(Query_input)\n",
        "      K = self.Wk(Key_input)\n",
        "      V = self.Wv(Value_input)\n",
        "\n",
        "      ### Projecting to create values for all the heads.\n",
        "      Q_projected,K_projected,V_projected = self.project_to_heads(Q,K,V);\n",
        "\n",
        "      ### Applying attention\n",
        "\n",
        "      Attention_output = self.dot_product_attention(Q_projected, K_projected, V_projected, Mask)\n",
        "      ### (Batch_size x head x sequence_length x vector_dimension)\n",
        "\n",
        "      ### Now, we have weighted value vectors from all the heads, so, need to merge all of them first,\n",
        "      ### So, we will do reverse of what we used to seperate them, for all heads -> we will add up all the projections.\n",
        "\n",
        "      ### First transpose:\n",
        "      Attention_out_transposed = tf.transpose(Attention_output, perm=[0,2,1,3])\n",
        "      ### Dimension: (Batch_size x sequence_length x head x vector_dimension) -> We have all the projected vectors on the penultimate axis -> now we merge\n",
        "      Attention_out_reshaped = tf.reshape(Attention_out_transposed, shape=(Attention_out_transposed.shape[0],Attention_out_transposed.shape[1],-1))\n",
        "      ### Dimension: (Batch_size x sequence_length x vector_dimension)\n",
        "\n",
        "\n",
        "      ### We have to produce output for the layer as: (Batch_size x sequence_length x d_model) as that's the output for every layer, and learn the attention from all heads,\n",
        "\n",
        "      ## So, we use another learnable weight:\n",
        "\n",
        "      Output = self.Wo(Attention_out_reshaped)\n",
        "\n",
        "      ### Shape:  (Batch_size x sequence_length x d_model) as we have already explained and Wo is initialized with d_model.\n",
        "\n",
        "      return Output\n",
        "\n",
        "\n",
        "\n",
        "class FeedForwardLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,d_ff):\n",
        "    super(FeedForwardLayer, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_ff = d_ff\n",
        "    self.layer_1=tf.keras.layers.Dense(d_ff, activation= \"relu\")\n",
        "    self.layer_2=tf.keras.layers.Dense(d_model)\n",
        "\n",
        "\n",
        "  def call(self, input):\n",
        "\n",
        "    out_1 = self.layer_1(input)\n",
        "    out_2 = self.layer_2(out_1)\n",
        "\n",
        "    return out_2\n",
        "\n",
        "################################ ENCODER #####################################\n",
        "\n",
        "class Encoder_module(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, d_vector, heads):\n",
        "    super(Encoder_module, self).__init__()\n",
        "\n",
        "    self.attention_layer = MultiheadAttention(d_model,d_vector,heads)\n",
        "    self.feed_forward_layer = FeedForwardLayer(d_model,d_ff)\n",
        "    self.addition_layer = tf.keras.layers.Add()\n",
        "    self.normalization_layer = tf.keras.layers.BatchNormalization()\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "    ## Dropout only applied during training.\n",
        "\n",
        "\n",
        "  def call(self, input, padding_mask, training = False):\n",
        "\n",
        "    ## For encoder all 3 heads of the attention recieves the same input.\n",
        "    ### Padding mask is required as the sequence length should be same, padded 0's will be there, and can get considered in loss function\n",
        "\n",
        "    out_attention = self.attention_layer(input,input,input, padding_mask)\n",
        "    ## No mask in encoder\n",
        "\n",
        "    add_attention = self.addition_layer([input, out_attention])\n",
        "    norm_attention = self.normalization_layer(add_attention)\n",
        "\n",
        "    attention_dropout = self.dropout(norm_attention, training = training)\n",
        "\n",
        "    out_ff_layer = self.feed_forward_layer(attention_dropout)\n",
        "\n",
        "    add_ff_layer = self.addition_layer([attention_dropout, out_ff_layer])\n",
        "    norm_ff_layer = self.normalization_layer(add_ff_layer)\n",
        "\n",
        "    ff_dropout = self.dropout(norm_ff_layer, training = training)\n",
        "\n",
        "    return ff_dropout\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, d_vector, heads, No_of_encoders):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.encoders = [Encoder_module(d_model,d_ff,d_vector, heads) for i in range(No_of_encoders)]\n",
        "\n",
        "  def call(self, input, mask = None, training = False):\n",
        "\n",
        "    x = input\n",
        "    for encoder in self.encoders:\n",
        "      x = encoder(x,mask,training = training)\n",
        "\n",
        "    return x\n",
        "\n",
        "#################################################################################\n",
        "\n",
        "################################# DECODER #######################################\n",
        "\n",
        "\n",
        "\n",
        "class Decoder_module(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, d_vector, heads):\n",
        "    super(Decoder_module, self).__init__()\n",
        "\n",
        "    self.attention_layer_input_from_previous_decoder_layer = MultiheadAttention(d_model,d_vector,heads)\n",
        "    self.attention_layer_input_from_encoder_output = MultiheadAttention(d_model,d_vector,heads)\n",
        "\n",
        "    ### In decoder module we have 2 attention layer: 1 where all query, key and value comes from the last decoder layer, and attention attends to the last\n",
        "    ### decoder layer only.\n",
        "    ### The other layer, where query and key comes from encoder output and value from decoder, and attention attends accordingly.\n",
        "\n",
        "    self.feed_forward_layer = FeedForwardLayer(d_model,d_ff)\n",
        "    self.addition_layer = tf.keras.layers.Add()\n",
        "    self.normalization_layer = tf.keras.layers.BatchNormalization()\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "\n",
        "  def call(self, input, encoder_output, padding_mask, look_ahead_mask, training = False):\n",
        "\n",
        "    ### In decoders masking will be used for each self attention,\n",
        "\n",
        "    ### In decoders two types of masks are used:\n",
        "    ### 1. The first attention layer accepts the input sentence itself while training as input to the decoder, there we need to add look-ahead mask, so\n",
        "    ### the model does not attend to the succeeding word tokens\n",
        "\n",
        "    ### 2. The second attention layer accepts input from the encoder output which has 0 padding, so padding mask is used there to remove the 0s from contributing\n",
        "    ### to the loss function\n",
        "\n",
        "    out_attention_1 = self.attention_layer_input_from_previous_decoder_layer(input,input,input,look_ahead_mask)\n",
        "\n",
        "    add_attention = self.addition_layer([input, out_attention_1])\n",
        "    norm_attention = self.normalization_layer(add_attention)\n",
        "\n",
        "    attention_dropout = self.dropout(norm_attention, training = training)\n",
        "\n",
        "    out_attention_2 = self.attention_layer_input_from_encoder_output(input,encoder_output,encoder_output,padding_mask)\n",
        "\n",
        "    add_attention_2 = self.addition_layer([attention_dropout, out_attention_2])\n",
        "    norm_attention_2 = self.normalization_layer(add_attention_2)\n",
        "\n",
        "    attention_dropout_2 = self.dropout(norm_attention_2, training = training)\n",
        "\n",
        "    out_ff_layer = self.feed_forward_layer(attention_dropout_2)\n",
        "\n",
        "    add_ff_layer = self.addition_layer([attention_dropout_2, out_ff_layer])\n",
        "    norm_ff_layer = self.normalization_layer(add_ff_layer)\n",
        "\n",
        "    ff_dropout = self.dropout(norm_ff_layer, training = training)\n",
        "\n",
        "    return ff_dropout\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, d_vector, heads, No_of_decoders):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.decoders = [Decoder_module(d_model,d_ff,d_vector, heads) for i in range(No_of_decoders)]\n",
        "\n",
        "  def call(self, input, encoder_output, padding_mask, look_ahead_mask, training = False):\n",
        "\n",
        "    x = input\n",
        "    for decoder in self.decoders:\n",
        "      x = decoder(x,encoder_output,padding_mask,look_ahead_mask, training)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "###########################################################################################################\n",
        "\n",
        "################################## POSITIONAL ENCODING ########################################\n",
        "\n",
        "class Positional_Encoding():\n",
        "\n",
        "  def __init__(self, d, sequence_length):\n",
        "\n",
        "    self.d = d\n",
        "    self.sequence_length = sequence_length\n",
        "    self.n = 10000 ## User defined variable -> 10000 decided by the authors\n",
        "\n",
        "\n",
        "  def get_positional_embeddings(self):\n",
        "\n",
        "    Positional_embedding_matrix = np.zeros(shape=(self.sequence_length, self.d))\n",
        "\n",
        "    ### d = d_model => postion_embedding gets added to the read embedding of the word, so size of the matrices should be same.\n",
        "    ### So, a d_model length vector for all the elements in the sequence.\n",
        "\n",
        "    for k in range(self.sequence_length):   #### k represents the postion in the sequence -> varies from [0,1,2.......... sequence_length-1]\n",
        "      for i in range(int(self.d/2)):  ### Projects the embedding value of k to dimension d based on the equations.\n",
        "        Positional_embedding_matrix[k,2*i] = np.sin(k/(np.power(self.n,2*i/self.d)))\n",
        "        Positional_embedding_matrix[k,2*i+1] = np.cos(k/(np.power(self.n,2*i/self.d)))\n",
        "\n",
        "    return Positional_embedding_matrix\n",
        "\n",
        "#############################################################################################################\n",
        "\n",
        "\n",
        "###################################### FULL TRANSFORMER ################################################\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, d_vector, heads, No_of_encoders, No_of_decoders, sequence_length, vocab_size):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_ff = d_ff\n",
        "    self.d_vector = d_vector\n",
        "    self.heads = heads\n",
        "    self.No_of_decoders = No_of_decoders\n",
        "    self.No_of_encoders = No_of_encoders\n",
        "    self.sequence_length = sequence_length\n",
        "    self.encoder = Encoder(d_model,d_ff,d_vector, heads, No_of_encoders)\n",
        "    self.decoder = Decoder(d_model,d_ff,d_vector, heads, No_of_decoders)\n",
        "    self.position_encoding_vector = Positional_Encoding(d_model, sequence_length)\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size, activation = \"softmax\")\n",
        "\n",
        "    #### Transforming the d_model vector to vocab_size vector, so that we get the predicted word using softmax\n",
        "\n",
        "\n",
        "    ### Embedding layer takes the input vector with size l and provides a random unique vector of a defined length, which serves as the unique representation\n",
        "    ### of the given word.\n",
        "\n",
        "\n",
        "  def call(self, input, output, look_ahead_mask = None, encoder_padding_mask = None, decoder_padding_mask = None, training = False):\n",
        "\n",
        "    embedding_input = self.embedding_layer(input)\n",
        "\n",
        "    position_embeddings_for_batch = np.array([self.position_encoding_vector.get_positional_embeddings() for _ in range(input.shape[0])])\n",
        "\n",
        "    embedding_with_positional_encoding_input = embedding_input + tf.cast(position_embeddings_for_batch,dtype=\"float32\")\n",
        "\n",
        "    embedding_output = self.embedding_layer(output)\n",
        "\n",
        "    embedding_with_positional_encoding_output = embedding_output + tf.cast(position_embeddings_for_batch,dtype=\"float32\")\n",
        "\n",
        "    encoder_output = self.encoder(embedding_with_positional_encoding_input, encoder_padding_mask, training)\n",
        "\n",
        "    encoder_output_dropout = self.dropout(encoder_output, training = training)\n",
        "\n",
        "    decoder_output = self.decoder(embedding_with_positional_encoding_output, encoder_output_dropout, decoder_padding_mask, look_ahead_mask, training)\n",
        "\n",
        "    decoder_output_dropout = self.dropout(decoder_output, training = training)\n",
        "\n",
        "    prediction = self.dense(decoder_output_dropout)\n",
        "\n",
        "    return prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "kc1jZew3zrex"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Paramater assignments\n",
        "\n",
        "d_model = 256;   # Each word embedding size (1 x 256)\n",
        "No_of_encoders = 4;\n",
        "No_of_decoders = 4;\n",
        "\n",
        "d_ff = 512 # No of units in the internal Feed forward network layer\n",
        "\n",
        "vector_dimensions = 64 # Q,V and K are of dimension (1 x 64) => Wq, Wv and Wk dimension (256 x 64)\n",
        "No_of_heads = 4 # Multi head attention\n",
        "\n",
        "sequence_length = 30\n",
        "\n",
        "vocab_size = 100\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "transformer = Transformer(d_model,d_ff, vector_dimensions, No_of_heads, No_of_encoders, No_of_decoders, sequence_length, vocab_size)"
      ],
      "metadata": {
        "id": "MhDmv0-srhMh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_input = np.random.rand(batch_size, sequence_length)\n",
        "random_output = np.random.rand(batch_size, sequence_length)\n",
        "\n",
        "pred = transformer(random_input, random_output)\n",
        "\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px7KCB5lVCso",
        "outputId": "870ecd1f-3876-4309-805f-9fbab31f4cba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  1319680   \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  1583616   \n",
            "                                                                 \n",
            " embedding (Embedding)       multiple                  25600     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         multiple                  0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            multiple                  25700     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,954,596\n",
            "Trainable params: 2,950,500\n",
            "Non-trainable params: 4,096\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating masks:\n",
        "\n",
        "def create_look_ahead_mask(sequence_length):\n",
        "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "\"\"\"\n",
        "array([[[1., 0., 0.],\n",
        "        [1., 1., 0.],\n",
        "        [1., 1., 1.]]],\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#### The mask creates a upper traingular matrix, which is substracted from 1 to reverse the 1's to 0's and 0's to -1's which are then added to make the dot product values\n",
        "### miniumum.\n",
        "\n",
        "def create_padding_mask(input):\n",
        "    padding_mask = 1 - np.equal(input, 0)\n",
        "    return tf.cast(padding_mask, dtype = \"float32\")\n",
        "\n",
        "### If input is 0 the mask has value 0."
      ],
      "metadata": {
        "id": "SRijCHlRWrAs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYE-VmX5JIRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}